{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tveKY1qNUiY3"
   },
   "source": [
    "## 📥 > 📙 Import to Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ztUox8JjUh62"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQym6szo-394"
   },
   "source": [
    "## 📋 Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_data(data_dir,width=224, height=224):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    dirs = os.listdir(data_dir)\n",
    "    dirs = [dir for dir in dirs if not dir.startswith ('.')] #.DS_Store 제외\n",
    "    for dir in dirs:\n",
    "        #print(dir) #paper\n",
    "        files = os.listdir(os.path.join(data_dir, dir))\n",
    "        files = [file for file in files if not file.startswith ('.')] #.DS_Store 제외\n",
    "        #files = files[:100] #데이터를 100개로 제한\n",
    "        for file in files:\n",
    "            #print(file) #8.jpg\n",
    "            image = Image.open(os.path.join(data_dir, dir, file))\n",
    "            image = image.resize((width, height))\n",
    "            image = np.array(image) #이미지 타입을 넘파이 타입으로 변환\n",
    "            x_data.append(image)\n",
    "            if dir == 'rock':\n",
    "                y_data.append(0)\n",
    "            elif dir == 'paper':\n",
    "                y_data.append(1)\n",
    "            elif dir == 'scissors':\n",
    "                y_data.append(2)\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    return x_data, y_data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train = load_data('./datasets/human_data/train_data')\n",
    "#x_test, y_test = load_data('./datasets/human_data/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 3000\n"
     ]
    }
   ],
   "source": [
    "#print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slk9Z40hJ98G",
    "outputId": "156ecb82-04cd-4524-8ab0-c88ac5663fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=1.0/255.0,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    dtype=None)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=1.0/255.0,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    dtype=None)\n",
    "\n",
    "#train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#train_datagen.fit(x_train)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\"./datasets/human_data/train_data\",target_size=(128, 128),\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    interpolation=\"nearest\")\n",
    "\n",
    "#test_datagen.fit(x_test)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\"./datasets/human_data/test_data\",target_size=(128, 128),\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrxHiiwY-xDV"
   },
   "source": [
    "## 🧱 Models Structure and Code [Function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tbYGWaUb6Gkp"
   },
   "outputs": [],
   "source": [
    "def func(pre,name_model):\n",
    "    print('#####~Model => {} '.format(name_model))\n",
    "    pre_model = name_model(input_shape=(128,128, 3),\n",
    "                   include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   pooling='avg')\n",
    "    pre_model.trainable = False\n",
    "    inputs = pre_model.input\n",
    "    x = Dense(64, activation='relu')(pre_model.output)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(15, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, mode='auto')\n",
    "    \n",
    "    history = model.fit(train_generator,validation_data=test_generator,epochs=5,callbacks=es,verbose=1)\n",
    "    # Plotting Accuracy, val_accuracy, loss, val_loss\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict Data Test\n",
    "    pred = model.predict(test_generator)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "    \n",
    "    print('\\033[01m              Classification_report \\033[0m')\n",
    "    \n",
    "    print('\\033[01m              Results \\033[0m')\n",
    "    # Results\n",
    "    results = model.evaluate(test_generator, verbose=1)\n",
    "    print(\"    Test Loss:\\033[31m \\033[01m {:.5f} \\033[30m \\033[0m\".format(results[0]))\n",
    "    print(\"Test Accuracy:\\033[32m \\033[01m {:.2f}% \\033[30m \\033[0m\".format(results[1] * 100))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aQfN3sSV2B8e"
   },
   "outputs": [],
   "source": [
    "def emir_model():\n",
    "    inp = Input(shape = (128,128,3))\n",
    "    x = Conv2D(32, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(32, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = Conv2D(128, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = Conv2D(256, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = Conv2D(512, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = Conv2D(1024, (3,3), strides=(2, 2), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(512, (3,3), strides=(1, 1), padding='same', activation='relu', use_bias=True)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(15, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs= x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ug-8awarRgT2"
   },
   "outputs": [],
   "source": [
    "def emirhan_func(name_model):\n",
    "\n",
    "    print('#####~Model => {} '.format(name_model))\n",
    "\n",
    "    model = emir_model()\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    my_callbacks  = [EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              mode='auto')]\n",
    "    \n",
    "    history = model.fit(train_generator,\n",
    "                        validation_data=test_generator,\n",
    "                        epochs=64,\n",
    "                        callbacks=my_callbacks,\n",
    "                        verbose=1,\n",
    "                        batch_size=128,)\n",
    "    # Plotting Accuracy, val_accuracy, loss, val_loss\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict Data Test\n",
    "    pred = model.predict(test_generator)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "    \n",
    "    print('\\033[01m              Classification_report \\033[0m')\n",
    "    \n",
    "    print('\\033[01m              Results \\033[0m')\n",
    "    # Results\n",
    "    results = model.evaluate(test_generator, verbose=1)\n",
    "    print(\"    Test Loss:\\033[31m \\033[01m {:.5f} \\033[30m \\033[0m\".format(results[0]))\n",
    "    print(\"Test Accuracy:\\033[32m \\033[01m {:.2f}% \\033[30m \\033[0m\".format(results[1] * 100))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O2NGUNEexwCY"
   },
   "outputs": [],
   "source": [
    "def depth(x, strides):\n",
    "    x = DepthwiseConv2D(3,strides=strides,padding='same',  use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "def single_conv_block(x,filters):\n",
    "    x = Conv2D(filters,1,use_bias=False)(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = SpatialDropout2D(0.2)(x)\n",
    "    return x\n",
    "def hidden_layers(x,filters,strides):\n",
    "    x = depth(x,strides)\n",
    "    x = single_conv_block(x, filters)\n",
    "    return x\n",
    "def EmirhanModel(input_shape=(128,128,3),n_classes = 15):\n",
    "    input = Input (input_shape)\n",
    "    x = Conv2D(32,3,strides=(2,2),padding = 'same', use_bias=False) (input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = ZeroPadding2D(padding=(2, 2))(x)\n",
    "    x = hidden_layers(x,64, strides=(1,1))\n",
    "    x = hidden_layers(x,128,strides=(2,2))\n",
    "    x = hidden_layers(x,128,strides=(1,1))\n",
    "    x = hidden_layers(x,256,strides=(2,2))\n",
    "    x = hidden_layers(x,256,strides=(1,1))\n",
    "    x = hidden_layers(x,512,strides=(2,2))\n",
    "    for _ in range(5):\n",
    "        x = hidden_layers(x,512,strides=(1,1))\n",
    "        x = hidden_layers(x,1024,strides=(2,2))\n",
    "        x = hidden_layers(x,1024,strides=(1,1))\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        output = Dense(n_classes,activation='softmax')(x)\n",
    "        model = Model(input, output)\n",
    "        return model\n",
    "number_of_classes = 15\n",
    "input_shape = (128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DWJrJSEyYwcW"
   },
   "outputs": [],
   "source": [
    "def emirhan(name_model):\n",
    "\n",
    "    print('#####~Model => {} '.format(name_model))\n",
    "\n",
    "    model = EmirhanModel(input_shape,number_of_classes)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    my_callbacks  = [EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              mode='auto')]\n",
    "    \n",
    "    history = model.fit(train_generator,\n",
    "                        validation_data=test_generator,\n",
    "                        epochs=64,\n",
    "                        callbacks=my_callbacks,\n",
    "                        verbose=1,\n",
    "                        batch_size=128,)\n",
    "    # Plotting Accuracy, val_accuracy, loss, val_loss\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(history.history[met])\n",
    "        ax[i].plot(history.history['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict Data Test\n",
    "    pred = model.predict(test_generator)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "    \n",
    "    print('\\033[01m              Classification_report \\033[0m')\n",
    "    \n",
    "    print('\\033[01m              Results \\033[0m')\n",
    "    # Results\n",
    "    results = model.evaluate(test_generator, verbose=1)\n",
    "    print(\"    Test Loss:\\033[31m \\033[01m {:.5f} \\033[30m \\033[0m\".format(results[0]))\n",
    "    print(\"Test Accuracy:\\033[32m \\033[01m {:.2f}% \\033[30m \\033[0m\".format(results[1] * 100))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxrVKHKbtMYs"
   },
   "source": [
    "## 🏃‍♂️ Prep Models and My Model Benchmark Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_giDE7juJUd"
   },
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "bxoxknJTtu8-",
    "outputId": "e54da035-8ddf-47d4-fe0f-4c7494eb9bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####~Model => <function VGG19 at 0x000001A8EF47D2D0> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ledu2\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1663: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\ledu2\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1671: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "118/118 [==============================] - 646s 5s/step - loss: 2.3874 - accuracy: 0.2515 - val_loss: 2.1362 - val_accuracy: 0.3057\n",
      "Epoch 2/5\n",
      "118/118 [==============================] - 643s 5s/step - loss: 1.8952 - accuracy: 0.3849 - val_loss: 1.9574 - val_accuracy: 0.3460\n",
      "Epoch 3/5\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7544 - accuracy: 0.4265"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "result_VGG19 = func(preprocess_input,VGG19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwxzsSsruO2X"
   },
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "eY4zBbvEt1H2",
    "outputId": "6aa0a6d7-71a4-48e0-a95e-849204e01f42"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "result_VGG16 = func(preprocess_input,VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CoNYxrmuXIE"
   },
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "nK3G3ujHuWlc",
    "outputId": "75ce52e0-68b2-46a6-a417-4fb51e2f536e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "result_ResNet50 = func(preprocess_input,ResNet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUF6ZYCsugkO"
   },
   "source": [
    "### ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "CIbgaYkeufZw",
    "outputId": "e094d097-4477-41b1-96a2-5ecdd457e2ad"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "result_ResNet101 = func(preprocess_input,ResNet101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEvRLYk0vGmi"
   },
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "WOyByv7UvFl9",
    "outputId": "14448672-d2c3-4e33-8ad1-07eae93eab29"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "result_MobileNet = func(preprocess_input,MobileNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCzitLzRw0dY"
   },
   "source": [
    "### DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "X7uq1iwAw0Gl",
    "outputId": "3eff8b2b-28e0-4e7f-f8bd-8ae9f3355712"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "result_DenseNet201 = func(preprocess_input,DenseNet201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIPgKZwVuQs4"
   },
   "source": [
    "### EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "pogTMEULfHIq",
    "outputId": "a4ffd0dd-19b9-49a7-87ba-440c5a22282b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "result_Eff = func(preprocess_input,EfficientNetB7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnRgLYDZw6NH"
   },
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "3axFrFQgw5nJ",
    "outputId": "c6c4ac9a-fd02-4525-abf2-3f7749efdc9c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "result_Xception = func(preprocess_input,Xception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTfd-l3YwvEo"
   },
   "source": [
    "### InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "bPbWzdiLwuaN",
    "outputId": "6529ed32-d7ad-4502-c0c0-ddb50b6f19e3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
    "result_InResNetV2 = func(preprocess_input,InceptionResNetV2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_Xpi8WN-kBd"
   },
   "source": [
    "### Model Emirhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z18CXqJL6kml",
    "outputId": "0e2927b7-f027-4313-e2e8-21e01bb812c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Emirhan_Human_Action_Detection_with_Artificial_Intelligence\"\n",
    "result_emirhan = emirhan_func(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msmvFgrd7-4h"
   },
   "source": [
    "## 📊 Finally Result of Table (DataFrame - Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXAc_cPy7pMD"
   },
   "outputs": [],
   "source": [
    "accuracy_result_table = pd.DataFrame({'Model':['Emirhan_Model','VGG16','VGG19','ResNet50','ResNet101','MobileNet','InceptionResNetV2',\n",
    "                               'DenseNet201','Xception','EfficientNetB7'],\n",
    "                      'Accuracy':[result_emirhan[1],result_VGG16[1], result_VGG19[1], result_ResNet50[1], result_ResNet101[1],\n",
    "                                  result_MobileNet[1],result_InResNetV2[1],result_DenseNet201[1],result_Xception[1],\n",
    "                                 result_Eff[1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "hTJSLRKS712R",
    "outputId": "80b2a372-31d0-4e26-f183-249f95d27107",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "wSd3aT7QULoB",
    "outputId": "88af09c2-aa4b-4a67-a920-4593d23e4d9b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plots = sns.barplot(x='Model', y='Accuracy', data=accuracy_result_table)\n",
    "for bar in plots.patches:\n",
    "    plots.annotate(format(bar.get_height(), '.2f'),\n",
    "                   (bar.get_x() + bar.get_width() / 2,\n",
    "                    bar.get_height()), ha='center', va='center',\n",
    "                   size=15, xytext=(0, 9),\n",
    "                   textcoords='offset points')\n",
    "\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aKrKJeKQMiW"
   },
   "outputs": [],
   "source": [
    "loss_result_table = pd.DataFrame({'Model':['Emirhan_Model','VGG16','VGG19','ResNet50','ResNet101','MobileNet','InceptionResNetV2',\n",
    "                               'DenseNet201','Xception','EfficientNetB7'],\n",
    "                      'Loss':[result_emirhan[0],result_VGG16[0], result_VGG19[0], result_ResNet50[0], result_ResNet101[0],\n",
    "                                  result_MobileNet[0],result_InResNetV2[0],result_DenseNet201[0],result_Xception[0],\n",
    "                                 result_Eff[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Gsxfa0V4QXcA",
    "outputId": "f2a3e547-bda5-4c6a-b55d-b47f437d80eb"
   },
   "outputs": [],
   "source": [
    "loss_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "RWAzDPdHUOcT",
    "outputId": "5d3faa4b-554f-408c-e6e9-bdae72cbadb9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plots = sns.barplot(x='Model', y='Loss', data=loss_result_table)\n",
    "for bar in plots.patches:\n",
    "    plots.annotate(format(bar.get_height(), '.2f'),\n",
    "                   (bar.get_x() + bar.get_width() / 2,\n",
    "                    bar.get_height()), ha='center', va='center',\n",
    "                   size=15, xytext=(0, 9),\n",
    "                   textcoords='offset points')\n",
    "\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(rotation=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Human Action Detection with Artificial Intelligence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9841de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6289789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "emotion = []\n",
    "\n",
    "with open('pos_pol_word.txt', encoding = \"UTF-8\") as pos:\n",
    "    for idx, line in enumerate(pos):\n",
    "        if idx > 18:\n",
    "            word_list.append(line.strip())\n",
    "            emotion.append('2')\n",
    "            \n",
    "with open('neg_pol_word.txt', encoding = \"UTF-8\") as pos:\n",
    "    for idx, line in enumerate(pos):\n",
    "        if idx > 18:\n",
    "            word_list.append(line.strip())\n",
    "            emotion.append('0')\n",
    "            \n",
    "            \n",
    "with open('obj_unknown_pol_word.txt', encoding = \"UTF-8\") as pos:\n",
    "    for idx, line in enumerate(pos):\n",
    "        if idx > 19:\n",
    "            word_list.append(line.strip())\n",
    "            emotion.append('1')\n",
    "            \n",
    "emotion = [int(x) for x in emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9417986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_pd = pd.DataFrame(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d9cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리\n",
    "# Tokenizer 객체 생성\n",
    "# num_words: 사용할 단어의 최대 개수 (가장 빈도가 높은 단어부터 사용)\n",
    "# oov_token: 단어 사전에 없는 단어를 대체할 토큰\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "\n",
    "# 데이터에 있는 텍스트를 기반으로 단어 사전 생성\n",
    "tokenizer.fit_on_texts(word_list)\n",
    "\n",
    "# 텍스트를 단어 사전에 따라 숫자 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(word_list)\n",
    "\n",
    "# 패딩 처리\n",
    "# maxlen: 시퀀스의 최대 길이\n",
    "# padding: 'post'는 시퀀스 뒤에 패딩을 추가, 'pre'는 앞에 추가\n",
    "# truncating: 'post'는 시퀀스 뒤를 잘라냄, 'pre'는 앞을 잘라냄\n",
    "padded = pad_sequences(sequences, maxlen=10, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d1bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 모델 생성\n",
    "model = Sequential([\n",
    "    # Embedding 레이어: 단어를 고정 크기의 벡터로 임베딩\n",
    "    # 10000: 단어 사전의 크기\n",
    "    # 16: 임베딩 벡터의 차원\n",
    "    # input_length=10: 입력 시퀀스의 길이\n",
    "    Embedding(10000, 16, input_length=10),\n",
    "\n",
    "    # LSTM 레이어: 순환 신경망의 한 종류로, 시퀀스 데이터 처리에 적합\n",
    "    # 32: LSTM 유닛의 개수\n",
    "    LSTM(32),\n",
    "\n",
    "    # Dense 레이어: 완전 연결 레이어\n",
    "    # 24: 뉴런의 개수\n",
    "    # activation='relu': 활성화 함수로 ReLU 사용\n",
    "    Dense(24, activation='relu'),\n",
    "\n",
    "    # Dropout 레이어: 과적합 방지를 위해 일부 뉴런을 무작위로 비활성화\n",
    "    # 0.5: 비활성화할 뉴런의 비율\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Dense 레이어: 출력 레이어\n",
    "    # 3: 출력 클래스의 개수 (긍정, 부정, 중립)\n",
    "    # activation='softmax': 다중 분류를 위한 활성화 함수\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "# loss='sparse_categorical_crossentropy': 다중 분류 손실 함수\n",
    "# optimizer='adam': 최적화 알고리즘으로 Adam 사용\n",
    "# metrics=['accuracy']: 정확도를 평가 지표로 사용\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee237dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "465/465 [==============================] - 3s 4ms/step - loss: 0.6734 - accuracy: 0.6934\n",
      "Epoch 2/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.2065 - accuracy: 0.9463\n",
      "Epoch 3/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0930 - accuracy: 0.9782\n",
      "Epoch 4/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0704 - accuracy: 0.9818\n",
      "Epoch 5/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0563 - accuracy: 0.9834\n",
      "Epoch 6/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0465 - accuracy: 0.9837\n",
      "Epoch 7/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0423 - accuracy: 0.9851\n",
      "Epoch 8/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0384 - accuracy: 0.9856\n",
      "Epoch 9/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0336 - accuracy: 0.9859\n",
      "Epoch 10/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0360 - accuracy: 0.9867\n",
      "Epoch 11/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0310 - accuracy: 0.9871\n",
      "Epoch 12/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0357 - accuracy: 0.9867\n",
      "Epoch 13/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0359 - accuracy: 0.9873\n",
      "Epoch 14/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0286 - accuracy: 0.9878\n",
      "Epoch 15/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0259 - accuracy: 0.9871\n",
      "Epoch 16/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0303 - accuracy: 0.9881\n",
      "Epoch 17/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0309 - accuracy: 0.9881\n",
      "Epoch 18/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0277 - accuracy: 0.9886\n",
      "Epoch 19/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0261 - accuracy: 0.9887\n",
      "Epoch 20/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0269 - accuracy: 0.9890\n",
      "Epoch 21/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0256 - accuracy: 0.9895\n",
      "Epoch 22/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0284 - accuracy: 0.9898\n",
      "Epoch 23/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0257 - accuracy: 0.9908\n",
      "Epoch 24/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0252 - accuracy: 0.9906\n",
      "Epoch 25/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0226 - accuracy: 0.9912\n",
      "Epoch 26/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0243 - accuracy: 0.9919\n",
      "Epoch 27/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0227 - accuracy: 0.9914\n",
      "Epoch 28/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0218 - accuracy: 0.9921\n",
      "Epoch 29/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0193 - accuracy: 0.9929\n",
      "Epoch 30/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0190 - accuracy: 0.9933\n",
      "Epoch 31/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 32/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0171 - accuracy: 0.9939\n",
      "Epoch 33/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0161 - accuracy: 0.9943\n",
      "Epoch 34/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0184 - accuracy: 0.9940\n",
      "Epoch 35/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0159 - accuracy: 0.9948\n",
      "Epoch 36/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0160 - accuracy: 0.9947\n",
      "Epoch 37/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0130 - accuracy: 0.9947\n",
      "Epoch 38/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0133 - accuracy: 0.9939\n",
      "Epoch 39/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0132 - accuracy: 0.9951\n",
      "Epoch 40/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0138 - accuracy: 0.9950\n",
      "Epoch 41/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0130 - accuracy: 0.9954\n",
      "Epoch 42/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0133 - accuracy: 0.9954\n",
      "Epoch 43/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0116 - accuracy: 0.9959\n",
      "Epoch 44/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0110 - accuracy: 0.9958\n",
      "Epoch 45/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0098 - accuracy: 0.9957\n",
      "Epoch 46/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0112 - accuracy: 0.9955\n",
      "Epoch 47/50\n",
      "465/465 [==============================] - 2s 5ms/step - loss: 0.0096 - accuracy: 0.9967\n",
      "Epoch 48/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0098 - accuracy: 0.9958\n",
      "Epoch 49/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0097 - accuracy: 0.9960\n",
      "Epoch 50/50\n",
      "465/465 [==============================] - 2s 4ms/step - loss: 0.0077 - accuracy: 0.9968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e774ebf340>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 학습\n",
    "model.fit(padded, emotion_pd, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbee7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = ['거북하다', '각별하다', '행복하다', '가치있는']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17280f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 단어 사전에 따라 숫자 시퀀스로 변환\n",
    "new_sequences = tokenizer.texts_to_sequences(sample_data)\n",
    "\n",
    "# 패딩 처리\n",
    "# maxlen: 시퀀스의 최대 길이\n",
    "# padding: 'post'는 시퀀스 뒤에 패딩을 추가, 'pre'는 앞에 추가\n",
    "# truncating: 'post'는 시퀀스 뒤를 잘라냄, 'pre'는 앞을 잘라냄\n",
    "new_padded = pad_sequences(new_sequences, maxlen=10, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed9b3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 2.5275311e-36 3.7102641e-09]\n",
      " [2.8788704e-11 6.0516418e-18 1.0000000e+00]\n",
      " [2.2766038e-05 4.7413682e-08 9.9997723e-01]\n",
      " [5.1207597e-11 1.5418079e-17 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35798568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "'거북하다'의 예측 감정: 긍정\n",
      "\n",
      "'각별하다'의 예측 감정: 중립\n",
      "\n",
      "'행복하다'의 예측 감정: 중립\n",
      "\n",
      "'가치있는'의 예측 감정: 중립\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측 수행\n",
    "#model = load_model('emotion_model.h5')\n",
    "predictions = model.predict(new_padded)\n",
    "#new_model = tf.keras.models.load_model('emotion_model.h5')\n",
    "# 예측 결과 출력\n",
    "for i, pred in enumerate(predictions):\n",
    "    predicted_label = np.argmax(pred)\n",
    "    if predicted_label == 0:\n",
    "        print(f\"'{sample_data[i]}'의 예측 감정: 긍정\")\n",
    "        print()\n",
    "    elif predicted_label == 1:\n",
    "        print(f\"'{sample_data[i]}'의 예측 감정: 부정\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"'{sample_data[i]}'의 예측 감정: 중립\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05ced7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
